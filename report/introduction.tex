\section{Introduction \& Background}

\subsection{Protein Function Annotation Challenge}

Protein function annotation is critical for drug discovery, precision medicine, and disease understanding. Yet UniProt contains over 200 million sequences with fewer than 1\% experimentally characterized \cite{uniprot2023}. Traditional experimental validation costs exceed \$50,000 per protein and requires months to years, creating an urgent need for computational methods \cite{radivojac2013}.

Large language models (LLMs) promise scalable annotation but fail on out-of-distribution (OOD) proteins—precisely those most relevant for clinical applications involving novel variants and uncharacterized disease proteins \cite{wu2025rethinking}. This motivates retrieval-augmented approaches that explicitly ground predictions in existing knowledge.

\subsection{Critical Flaws in Existing Benchmarks}

Wu et al.~\cite{wu2025rethinking} revealed severe data leakage in protein function benchmarks: UniProtQA-Protein Family shows 97.7\% contamination, Mol-Instructions 30--80\%, and Swiss-Prot Caption 45.2\%. Test proteins sharing >30\% sequence identity with training data allow trivial homology transfer, masking true generalization failure.

The \textbf{Prot-Inst-OOD} dataset addresses this by enforcing <30\% sequence identity between train/test splits across four tasks: \textit{Catalytic Activity} (enzymatic mechanisms), \textit{Domain/Motif} (structural features), \textit{Protein Function} (biological processes), and \textit{General Function} (comprehensive characterization).

\subsection{Inadequate Evaluation Metrics}

Standard ROUGE/BLEU metrics measure text similarity rather than biological correctness. A prediction stating ``ABC transporter domains'' (correct biology) may score 0.27 while ``GGDEF, MHYT, EAL domains'' (wrong biology) scores 0.83 due to template matching.

\textbf{Meta-BLEU} (Entity-BLEU) solves this by: (1) extracting biological entities from predictions and ground truth, (2) converting to entity sequences, (3) computing BLEU on entity n-grams. This focuses evaluation on biological accuracy:
\begin{equation}
\text{Meta-BLEU} = BP \times \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\end{equation}
where $p_n$ is entity n-gram precision.

\subsection{RAPM Framework}

Using Prot-Inst-OOD and Meta-BLEU, Wu et al.~\cite{wu2025rethinking} showed that \textbf{Retrieval-Augmented Protein Modeling (RAPM)} outperforms fine-tuned LLMs:

\begin{enumerate}
    \item \textbf{Embedding}: Extract ESM-2 (1280-dim) sequence representations \cite{lin2023evolutionary}
    \item \textbf{Retrieval}: Find top-$K$ similar proteins via cosine similarity
    \item \textbf{Prompting}: Construct prompts with query, retrieved annotations, and few-shot examples
    \item \textbf{Generation}: LLM synthesizes functional description (no fine-tuning)
\end{enumerate}

RAPM avoids overfitting by grounding predictions in explicit examples, achieving Meta-BLEU-2 scores of 28.09 (Catalytic Activity), 16.09 (Domain/Motif), 47.40 (Protein Function), 4.78 (General Function).

\subsection{SAPER: Our Contribution}

RAPM's reliance on sequence-only embeddings ignores a fundamental principle: protein function depends on 3D structure. Anfinsen's hypothesis states structure determines function \cite{anfinsen1973}, but the converse fails—different sequences can fold into similar structures (convergent evolution) and share function despite low sequence identity.

For catalytic activity, active site geometry—not sequence—determines substrate specificity. For domains, structural motifs like Rossmann folds appear across sequences with <20\% identity. ESM-2 misses these structural homologs.

We present \textbf{SAPER (Structurally-Aware Prompt-Enhanced RAPM)} with three contributions:

\begin{enumerate}
    \item \textbf{Structural Embeddings}: ProstT5 \cite{heinzinger2023prostt5} generates 1024-dim structure-aware embeddings via 3Di structural alphabets, capturing geometric patterns complementary to ESM-2

    \item \textbf{Multi-Modal Fusion}: Reciprocal Rank Fusion (RRF) and Weighted Similarity Fusion ($\alpha=0.7$) combine structure/sequence retrieval

    \item \textbf{Enhanced Prompting}: Task-specific instructions, confidence signals, and structured formatting optimize Gemini 2.5 Flash performance
\end{enumerate}

SAPER achieves +114\% Meta-BLEU-2 improvement on Domain/Motif (16.09 $\rightarrow$ 34.43) and +68.1\% average improvement across all tasks, validating that structural information is critical for OOD generalization.
