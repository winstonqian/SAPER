\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% Use preprint option to show authors and remove anonymization
\usepackage[preprint]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%    \usepackage[final]{neurips_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % for figures
\usepackage{amsmath}        % for equations

% SPACING CONFIGURATION - Add double spacing
\usepackage{setspace}
\doublespacing % Explicitly double-spaced

\title{SAPER: Structurally-Aware Prompt-Enhanced Retrieval Augmented Protein Modeling Framework}

\author{
  Linrui Ma \\
  Massachusetts Institute of Technology \\
  \texttt{linrui@mit.edu} \\
  \And
  Winston Qian \\
  Harvard University \\
  \texttt{winstonqian@g.harvard.edu} \\
  \And
  Yiwei Liang \\
  Massachusetts Institute of Technology \\
  \texttt{liangyw@mit.edu} \\
  \And
  Emma Wang \\
  Massachusetts Institute of Technology \\
  \texttt{ewang13@mit.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Accurate protein function annotation is essential for drug discovery and precision medicine, yet large language models (LLMs) struggle with out-of-distribution (OOD) proteins. We present SAPER (Structurally-Aware Prompt-Enhanced RAPM), extending the retrieval-augmented RAPM framework with three key innovations: (1) ProstT5 structure-aware embeddings capturing 3D geometric features complementary to sequence-based ESM-2, (2) multi-modal fusion via Reciprocal Rank Fusion and Weighted Similarity (optimal at $\alpha=0.7$ emphasizing structure), and (3) enhanced prompt engineering with task-specific instructions and confidence signals. Evaluated on Prot-Inst-OOD with strict OOD splits (<30\% sequence identity) using Gemini 2.5 Flash, SAPER achieves +114\% Meta-BLEU-2 improvement on Domain Motif identification (16.09 $\rightarrow$ 34.43) and +68.1\% average improvement across all tasks, validating that structural information is critical for accurate functional annotation of novel proteins.
\end{abstract}

\section{Introduction \& Background}

\subsection{Protein Function Annotation Challenge}

Protein function annotation is critical for drug discovery, precision medicine, and disease understanding. Yet UniProt contains over 200 million sequences with fewer than 1\% experimentally characterized \cite{uniprot2023}. Traditional experimental validation costs exceed \$50,000 per protein and requires months to years, creating an urgent need for computational methods \cite{radivojac2013}.

Large language models (LLMs) promise scalable annotation but fail on out-of-distribution (OOD) proteins—precisely those most relevant for clinical applications involving novel variants and uncharacterized disease proteins \cite{wu2025rethinking}. This motivates retrieval-augmented approaches that explicitly ground predictions in existing knowledge.

\subsection{Critical Flaws in Existing Benchmarks}

Wu et al.~\cite{wu2025rethinking} revealed severe data leakage in protein function benchmarks: UniProtQA-Protein Family shows 97.7\% contamination, Mol-Instructions 30--80\%, and Swiss-Prot Caption 45.2\%. Test proteins sharing >30\% sequence identity with training data allow trivial homology transfer, masking true generalization failure.

The \textbf{Prot-Inst-OOD} dataset addresses this by enforcing <30\% sequence identity between train/test splits across four tasks: \textit{Catalytic Activity} (enzymatic mechanisms), \textit{Domain Motif} (structural features), \textit{Protein Function} (biological processes), and \textit{General Function} (comprehensive characterization).

\subsection{Inadequate Evaluation Metrics}

Standard ROUGE/BLEU metrics measure text similarity rather than biological correctness. A prediction stating ``ABC transporter domains'' (correct biology) may score 0.27 while ``GGDEF, MHYT, EAL domains'' (wrong biology) scores 0.83 due to template matching.

\textbf{Meta-BLEU} (Entity-BLEU) solves this by: (1) extracting biological entities from predictions and ground truth, (2) converting to entity sequences, (3) computing BLEU on entity n-grams. This focuses evaluation on biological accuracy:
\begin{equation}
\text{Meta-BLEU} = BP \times \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\end{equation}
where $p_n$ is entity n-gram precision.

\subsection{RAPM Framework}

Using Prot-Inst-OOD and Meta-BLEU, Wu et al.~\cite{wu2025rethinking} showed that \textbf{Retrieval-Augmented Protein Modeling (RAPM)} outperforms fine-tuned LLMs:

\begin{enumerate}
    \item \textbf{Embedding}: Extract ESM-2 (1280-dim) sequence representations \cite{lin2023evolutionary}
    \item \textbf{Retrieval}: Find top-$K$ similar proteins via cosine similarity
    \item \textbf{Prompting}: Construct prompts with query, retrieved annotations, and few-shot examples
    \item \textbf{Generation}: LLM synthesizes functional description (no fine-tuning)
\end{enumerate}

RAPM avoids overfitting by grounding predictions in explicit examples, achieving Meta-BLEU-2 scores of 28.09 (Catalytic Activity), 16.09 (Domain Motif), 47.40 (Protein Function), 4.78 (General Function).

\subsection{SAPER: Our Contribution}

RAPM's reliance on sequence-only embeddings ignores a fundamental principle: protein function depends on 3D structure. Anfinsen's hypothesis states structure determines function \cite{anfinsen1973}, but the converse fails—different sequences can fold into similar structures (convergent evolution) and share function despite low sequence identity.

For catalytic activity, active site geometry—not sequence—determines substrate specificity. For domains, structural motifs like Rossmann folds appear across sequences with <20\% identity. ESM-2 misses these structural homologs.

We present \textbf{SAPER (Structurally-Aware Prompt-Enhanced RAPM)} with three contributions:

\begin{enumerate}
    \item \textbf{Structural Embeddings}: ProstT5 \cite{heinzinger2023prostt5} generates 1024-dim structure-aware embeddings via 3Di structural alphabets, capturing geometric patterns complementary to ESM-2

    \item \textbf{Multi-Modal Fusion}: Reciprocal Rank Fusion (RRF) and Weighted Similarity Fusion ($\alpha=0.7$) combine structure/sequence retrieval

    \item \textbf{Enhanced Prompting}: Task-specific instructions, confidence signals, and structured formatting optimize Gemini 2.5 Flash performance
\end{enumerate}

SAPER achieves +114\% Meta-BLEU-2 improvement on Domain Motif (16.09 $\rightarrow$ 34.43) and +68.1\% average improvement across all tasks, validating that structural information is critical for OOD generalization.

\section{Methods: Architecture and Key Improvements}

SAPER extends RAPM through four progressive methods evaluated on Prot-Inst-OOD \cite{wu2025rethinking} (256 randomly sampled test proteins per task, 3 runs with seeds 0, 42, 123). All methods follow a four-stage pipeline: (1) embedding extraction, (2) similarity-based retrieval (Top-$K=10$), (3) prompt construction, (4) LLM generation (Gemini 2.5 Flash, no fine-tuning).

\subsection{Method 1: Original RAPM Baseline}

Extract 1280-dim embeddings from ESM-2 \cite{lin2023evolutionary} (facebook/esm2\_t33\_650M\_UR50D):
\begin{equation}
\mathbf{e}_{\text{ESM-2}} = \text{ESM-2}_{\text{mean}}(s) \in \mathbb{R}^{1280}
\end{equation}

Retrieve Top-$K$ proteins via L2-normalized cosine similarity using Faiss HNSW indexing. Present confidence as High ($\geq 0.90$), Medium ($0.60$--$0.90$), or Low ($< 0.60$).

\subsection{Method 2: Hybrid Reciprocal Rank Fusion}

\subsubsection{ProstT5 Structural Embeddings}

ProstT5 \cite{heinzinger2023prostt5} is a bilingual model trained on sequences and 3Di structural alphabets from Foldseek \cite{vanKempen2024foldseek}. For protein $s$, predict structure, convert to 3Di tokens $s_{3Di}$, and extract 1024-dim embeddings:
\begin{equation}
\mathbf{e}_{\text{ProstT5}} = \text{ProstT5}_{\text{mean}}(s, s_{3Di}) \in \mathbb{R}^{1024}
\end{equation}

ProstT5 captures secondary structure ($\alpha$-helices, $\beta$-sheets), tertiary motifs (Rossmann folds, TIM barrels), and binding site geometries—complementary to ESM-2's evolutionary patterns.

\subsubsection{Reciprocal Rank Fusion (RRF)}

Retrieve Top-$K$ candidates independently with ProstT5 and ESM-2. Combine rankings via RRF \cite{cormack2009reciprocal}:
\begin{equation}
\text{RRF}_k(i) = \frac{1}{k + \text{rank}_{\text{ProstT5}}(i)} + \frac{1}{k + \text{rank}_{\text{ESM-2}}(i)}
\end{equation}
with $k=60$ (standard parameter). Select Top-$K$ by descending RRF score. This parameter-free fusion favors proteins ranking well in both modalities.

\subsection{Method 3: Weighted Similarity Fusion}

RRF discards similarity magnitudes. Weighted fusion directly combines normalized scores:
\begin{equation}
\text{Score}_{\alpha}(q, i) = \alpha \cdot \text{sim}_{\text{ProstT5}}(\mathbf{e}^{\text{P}}_q, \mathbf{e}^{\text{P}}_i) + (1 - \alpha) \cdot \text{sim}_{\text{ESM-2}}(\mathbf{e}^{\text{E}}_q, \mathbf{e}^{\text{E}}_i)
\end{equation}

where $\alpha \in [0, 1]$ controls structure/sequence balance. We use $\alpha=0.7$ (70\% structure, 30\% sequence) based on preliminary experiments, emphasizing structural similarity for OOD generalization—particularly in the sequence identity ``twilight zone'' (20--30\%) where structure-based methods excel.

\subsection{Method 4: Enhanced Prompt Engineering}

Building on Weighted Similarity ($\alpha=0.7$), we optimize prompts for Gemini 2.5 Flash through four enhancements:

\textbf{(1) Task-Specific Instructions:} Prime LLM with domain context. Example for Domain Motif: ``Identify structural domains, functional motifs, binding sites, and conserved sequence patterns.''

\textbf{(2) Confidence Signals:} Present retrieval with explicit confidence levels and scores:
\begin{verbatim}
[Rank 1] Confidence: High (0.87)
Annotation: {annotation_1}
\end{verbatim}
This enables LLM to weight evidence appropriately.

\textbf{(3) Few-Shot Examples:} Include 2--3 training examples (nearest neighbors to query) demonstrating desired output format and biological terminology.

\textbf{(4) JSON Constraints:} Require structured output (\texttt{\{"description": "..."\}}) for consistent parsing and reduced hallucination.

\subsection{Evaluation Metrics}

\textbf{Meta-BLEU} (primary): Extract biological entities from predictions/ground truth, compute BLEU on entity n-grams:
\begin{equation}
\text{Meta-BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\end{equation}
Report Meta-BLEU-2 (bigrams) and Meta-BLEU-4 (up to 4-grams) to capture multi-word terms like ``ATP-binding cassette transporter.''

\textbf{Meteor} (secondary): Semantic similarity with synonym matching \cite{banerjee2005meteor}:
\begin{equation}
\text{Meteor} = F_{\text{mean}} \cdot (1 - \text{Penalty})
\end{equation}

\subsection{Implementation Details}

\begin{itemize}
    \item \textbf{LLM}: Gemini 2.5 Flash (temperature=0.7, top\_p=0.9, max\_tokens=512)
    \item \textbf{Embeddings}: ESM-2 (1280-dim), ProstT5 (1024-dim), L2-normalized
    \item \textbf{Retrieval}: Faiss HNSW, Top-$K=10$
    \item \textbf{Evaluation}: 256 samples/task, 3 runs (seeds 0, 42, 123), averaged results
\end{itemize}

\section{Results}

We evaluate SAPER's four methods on Prot-Inst-OOD with strict OOD splits (<30\% sequence identity). All results average 3 runs (seeds 0, 42, 123) on 256 randomly sampled test proteins per task, using Gemini 2.5 Flash for LLM generation.

\subsection{Structural Embeddings and Fusion Methods}

Table~\ref{tab:fusion_results} shows performance across Original RAPM (Method 1), Hybrid RRF (Method 2), and Weighted Similarity $\alpha=0.7$ (Method 3). Figures~\ref{fig:meta_bleu2_comparison}--\ref{fig:meta_bleu4_comparison} visualize progressive improvements.

\begin{table}[h]
\centering
\caption{Performance comparison: fusion methods (average of 3 runs, 256 samples). Percentages show improvement over Original RAPM.}
\label{tab:fusion_results}
\small
\begin{tabular}{llccc}
\hline
\textbf{Task} & \textbf{Metric} & \textbf{Original} & \textbf{RRF} & \textbf{Weighted} \\
\hline
\multicolumn{5}{l}{\textit{Catalytic Activity}} \\
& Meta-BLEU-2 & 28.09 & 33.91 (+20.7\%) & \textbf{36.52 (+30.0\%)} \\
& Meta-BLEU-4 & 23.40 & 28.91 (+23.5\%) & \textbf{31.04 (+32.7\%)} \\
\hline
\multicolumn{5}{l}{\textit{Domain Motif}} \\
& Meta-BLEU-2 & 16.09 & 22.63 (+40.7\%) & \textbf{25.89 (+60.9\%)} \\
& Meta-BLEU-4 & 12.42 & 17.61 (+41.8\%) & \textbf{20.50 (+65.1\%)} \\
\hline
\multicolumn{5}{l}{\textit{Protein Function}} \\
& Meta-BLEU-2 & 47.40 & \textbf{49.80 (+5.1\%)} & 47.19 ($-$0.4\%) \\
& Meta-BLEU-4 & 37.52 & \textbf{39.12 (+4.3\%)} & 37.24 ($-$0.7\%) \\
\hline
\multicolumn{5}{l}{\textit{General Function}} \\
& Meta-BLEU-2 & 4.78 & 7.04 (+47.3\%) & \textbf{10.15 (+112.6\%)} \\
& Meta-BLEU-4 & 3.46 & 5.05 (+46.1\%) & \textbf{7.84 (+126.8\%)} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/meta_bleu2_comparison.png}
    \caption{Meta-BLEU-2 across methods. Structure-dependent tasks (Domain Motif, Catalytic Activity) show largest gains.}
    \label{fig:meta_bleu2_comparison}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/meta_bleu4_comparison.png}
    \caption{Meta-BLEU-4 across methods, mirroring Meta-BLEU-2 trends.}
    \label{fig:meta_bleu4_comparison}
\end{figure}

\textbf{Key Findings:} (1) Domain Motif shows largest improvement (+60.9\% for Weighted), validating that structure is critical for identifying 3D architecture. (2) Weighted Similarity outperforms RRF on 3/4 tasks, suggesting score magnitudes and structural emphasis ($\alpha=0.7$) benefit OOD generalization. (3) Protein Function favors RRF (+5.1\%), likely because evolutionary context dominates over geometry for this task. (4) All fusion methods outperform sequence-only baseline, demonstrating complementary information from structure+sequence.

\subsection{Enhanced Prompt Engineering}

Table~\ref{tab:prompt_results} compares Weighted Similarity with standard vs.~enhanced prompts (Method 4). Figures~\ref{fig:prompt_meta_bleu2}--\ref{fig:prompt_meta_bleu4} visualize additive benefits.

\begin{table}[h]
\centering
\caption{Effect of enhanced prompts on Weighted Similarity ($\alpha=0.7$). ``Relative'' shows total gain vs.~Original RAPM.}
\label{tab:prompt_results}
\small
\begin{tabular}{llcccc}
\hline
\textbf{Task} & \textbf{Metric} & \textbf{Weighted} & \textbf{Enhanced} & \textbf{Gain} & \textbf{Relative} \\
\hline
\multicolumn{6}{l}{\textit{Catalytic Activity}} \\
& Meta-BLEU-2 & 36.52 & \textbf{43.04} & +6.52 & \textbf{+53.2\%} \\
& Meta-BLEU-4 & 31.04 & \textbf{35.80} & +4.76 & \textbf{+53.0\%} \\
\hline
\multicolumn{6}{l}{\textit{Domain Motif}} \\
& Meta-BLEU-2 & 25.89 & \textbf{34.43} & +8.54 & \textbf{+114.0\%} \\
& Meta-BLEU-4 & 20.50 & \textbf{27.28} & +6.78 & \textbf{+119.6\%} \\
\hline
\multicolumn{6}{l}{\textit{Protein Function}} \\
& Meta-BLEU-2 & 47.19 & \textbf{56.66} & +9.47 & \textbf{+19.5\%} \\
& Meta-BLEU-4 & 37.24 & \textbf{47.15} & +9.91 & \textbf{+25.7\%} \\
\hline
\multicolumn{6}{l}{\textit{General Function}} \\
& Meta-BLEU-2 & 10.15 & \textbf{8.86} & $-$1.29 & \textbf{+85.5\%} \\
& Meta-BLEU-4 & 7.84 & \textbf{6.36} & $-$1.48 & \textbf{+83.9\%} \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/prompt_meta_bleu2_comparison.png}
    \caption{Enhanced prompts boost Meta-BLEU-2 on well-defined tasks (Catalytic Activity, Domain Motif, Protein Function).}
    \label{fig:prompt_meta_bleu2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/prompt_meta_bleu4_comparison.png}
    \caption{Meta-BLEU-4 improvements mirror Meta-BLEU-2 trends.}
    \label{fig:prompt_meta_bleu4}
\end{figure}

\textbf{Key Findings:} (1) Enhanced prompts deliver +6--9 Meta-BLEU-2 gains on Catalytic Activity, Domain Motif, and Protein Function. (2) Domain Motif achieves +114\% total improvement (Original $\rightarrow$ Enhanced), more than doubling performance. (3) Retrieval quality and prompt design synergize—better context enables better synthesis. (4) General Function shows slight decrease, suggesting heterogeneous tasks may need specialized prompt templates.

\subsection{Overall Summary}

SAPER (Enhanced Prompt + Weighted Similarity $\alpha=0.7$) achieves \textbf{+70.6\% average Meta-BLEU-4 improvement} across all tasks vs.~Original RAPM. Structure-dependent tasks show largest gains: Domain Motif +119.6\%, Catalytic Activity +53.0\%. Even sequence-centric Protein Function improves +25.7\%, demonstrating structural information provides complementary signal across diverse annotation challenges.

\section{Discussion}

\subsection{Why SAPER Works}

SAPER's task-specific gains reflect the structure-function paradigm in protein biology. Domain Motif identification (+114\% Meta-BLEU-2) benefits most because protein domains are defined by 3D architecture—spatial arrangements of secondary structure elements into folds like immunoglobulin domains or Rossmann folds. These structural motifs arise through convergent evolution in proteins with <20\% sequence identity, making them invisible to sequence-only retrieval. ProstT5's 3Di structural alphabet encodes local geometric environments, enabling recognition of distant structural homologs that ESM-2 misses.

Catalytic Activity (+53.2\%) similarly depends on active site geometry. Serine proteases across diverse families share Ser-His-Asp catalytic triads recognizable only through structural alignment, not sequence comparison. ProstT5 identifies these geometric similarities despite sequence divergence.

Conversely, Protein Function (+19.5\%) shows modest gains because annotations emphasize biological processes and pathway membership—features tied to evolutionary history. Sequence-based ESM-2 captures these relationships effectively, making structural emphasis ($\alpha=0.7$) less beneficial. This explains why RRF's equal weighting (+5.1\%) outperforms Weighted Similarity ($-$0.4\%) for this task.

Weighted Similarity's advantage over RRF stems from leveraging score magnitudes and emphasizing structure (70\% weight). In the sequence identity ``twilight zone'' (20--30\%) where homology detection fails, structural conservation extends further. By trusting ProstT5 matches more, Weighted Similarity improves retrieval for OOD proteins in this critical regime.

Enhanced prompts boost performance through: (1) task-specific instructions activating LLM domain knowledge, (2) confidence signals enabling weighted integration of evidence, (3) structured formatting reducing attention mechanism load, (4) JSON constraints eliminating hallucination. The synergy between retrieval quality and prompt design suggests multiplicative rather than additive effects—better context requires better presentation for optimal synthesis.

\subsection{Limitations}

\textbf{LLM Choice:} Gemini 2.5 Flash, used due to budget constraints (free API tier vs.~\$500--\$1000 for GPT-4), likely underestimates SAPER's full potential. Wu et al.~\cite{wu2025rethinking} achieved higher absolute scores with GPT-4. However, our focus on \textit{relative} improvements under identical conditions remains valid. The largest gains on structure-dependent tasks with a mid-tier LLM suggest trends would amplify with stronger models.

\textbf{Sample Size:} Evaluation on 256/task (3 runs) provides statistical robustness for large effect sizes (+114\%) but may underrepresent rare protein families or edge cases. Full-dataset evaluation would enable stratified analysis by sequence length, structural complexity, and sequence identity to nearest training protein.

\textbf{Fixed Hyperparameters:} $\alpha=0.7$ optimizes average performance but is suboptimal per-task. Protein Function prefers $\alpha \approx 0.5$, Domain Motif likely benefits from $\alpha > 0.7$. Task-specific or query-adaptive weighting could improve results.

\textbf{Prompt Design:} Enhanced prompts were manually designed for Gemini 2.5 Flash. Automated optimization (prompt tuning, LLM meta-prompting) could discover superior formulations and generalize across LLM families.

\textbf{Structural Embeddings:} ProstT5 relies on predicted structures (AlphaFold), propagating prediction errors. The discrete 3Di alphabet (20 states) discards fine-grained geometry like exact bond angles or long-range contacts. Alternative representations (3D coordinate GNNs, distance matrices) may capture richer information.

\subsection{Future Directions}

\textbf{State-of-the-Art LLMs:} Evaluate GPT-4 Turbo, Claude Opus 4.5, Llama 3.1 405B to assess whether relative improvements hold and absolute performance approaches expert levels. Model scaling analysis would quantify how SAPER benefits scale with LLM capability.

\textbf{Full Dataset:} Run on complete Prot-Inst-OOD test sets (thousands/task) for population-level estimates. Stratify by protein characteristics to identify where SAPER excels or struggles. Analyze confidence calibration—do retrieval scores correlate with accuracy?

\textbf{Adaptive Weighting:} Optimize $\alpha$ per-task via grid search, or develop instruction-based classifiers (keywords ``domain''/``motif'' $\rightarrow$ higher $\alpha$; ``pathway''/``process'' $\rightarrow$ lower $\alpha$). Query-adaptive weighting based on sequence identity to training set could further improve results.

\textbf{Multi-Modal Fusion:} Integrate AlphaFold 3D coordinates via GNNs, Gene Ontology semantic similarity, additional PLMs (ProtTrans, Ankh), and evolutionary features (MSA conservation, coevolution).

\textbf{Advanced Retrieval:} Learned similarity metrics (metric learning with GO supervision), cross-encoder re-ranking, graph-based retrieval (protein interaction networks), iterative refinement.

\textbf{Broader Applications:} Extend to protein-protein interaction prediction, mutation effect prediction (variant pathogenicity), drug-target binding, and de novo protein design.

\subsection{Conclusion}

SAPER demonstrates that incorporating structural information (ProstT5), optimizing multi-modal fusion (Weighted Similarity $\alpha=0.7$), and enhancing prompts substantially improves retrieval-augmented protein function prediction. Achieving +114\% improvement on Domain Motif and +68.1\% average improvement validates the structure-function paradigm and shows structurally-aware retrieval bridges the gap between exponential sequence data growth and slower functional characterization.

Despite limitations (Gemini 2.5 Flash, 256-sample evaluation), directional consistency and mechanistic interpretability provide a foundation for future work. SAPER exemplifies a broader principle in AI for science: \textit{domain-aware design outperforms generic methods}. By encoding biological knowledge—structure-function relationships, sequence identity twilight zones, active site geometry importance—into retrieval and prompts, we achieve performance gains that purely data-driven approaches miss.

% Bibliography
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
