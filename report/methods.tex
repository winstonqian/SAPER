\section{Methods: Architecture and Key Improvements}

SAPER extends RAPM through four progressive methods evaluated on Prot-Inst-OOD \cite{wu2025rethinking} (256 randomly sampled test proteins per task, 3 runs with seeds 0, 42, 123). All methods follow a four-stage pipeline: (1) embedding extraction, (2) similarity-based retrieval (Top-$K=10$), (3) prompt construction, (4) LLM generation (Gemini 2.5 Flash, no fine-tuning).

\subsection{Method 1: Original RAPM Baseline}

Extract 1280-dim embeddings from ESM-2 \cite{lin2023evolutionary} (facebook/esm2\_t33\_650M\_UR50D):
\begin{equation}
\mathbf{e}_{\text{ESM-2}} = \text{ESM-2}_{\text{mean}}(s) \in \mathbb{R}^{1280}
\end{equation}

Retrieve Top-$K$ proteins via L2-normalized cosine similarity using Faiss HNSW indexing. Present confidence as High ($\geq 0.90$), Medium ($0.60$--$0.90$), or Low ($< 0.60$).

\subsection{Method 2: Hybrid Reciprocal Rank Fusion}

\subsubsection{ProstT5 Structural Embeddings}

ProstT5 \cite{heinzinger2023prostt5} is a bilingual model trained on sequences and 3Di structural alphabets from Foldseek \cite{vanKempen2024foldseek}. For protein $s$, predict structure, convert to 3Di tokens $s_{3Di}$, and extract 1024-dim embeddings:
\begin{equation}
\mathbf{e}_{\text{ProstT5}} = \text{ProstT5}_{\text{mean}}(s, s_{3Di}) \in \mathbb{R}^{1024}
\end{equation}

ProstT5 captures secondary structure ($\alpha$-helices, $\beta$-sheets), tertiary motifs (Rossmann folds, TIM barrels), and binding site geometries—complementary to ESM-2's evolutionary patterns.

\subsubsection{Reciprocal Rank Fusion (RRF)}

Retrieve Top-$K$ candidates independently with ProstT5 and ESM-2. Combine rankings via RRF \cite{cormack2009reciprocal}:
\begin{equation}
\text{RRF}_k(i) = \frac{1}{k + \text{rank}_{\text{ProstT5}}(i)} + \frac{1}{k + \text{rank}_{\text{ESM-2}}(i)}
\end{equation}
with $k=60$ (standard parameter). Select Top-$K$ by descending RRF score. This parameter-free fusion favors proteins ranking well in both modalities.

\subsection{Method 3: Weighted Similarity Fusion}

RRF discards similarity magnitudes. Weighted fusion directly combines normalized scores:
\begin{equation}
\text{Score}_{\alpha}(q, i) = \alpha \cdot \text{sim}_{\text{ProstT5}}(\mathbf{e}^{\text{P}}_q, \mathbf{e}^{\text{P}}_i) + (1 - \alpha) \cdot \text{sim}_{\text{ESM-2}}(\mathbf{e}^{\text{E}}_q, \mathbf{e}^{\text{E}}_i)
\end{equation}

where $\alpha \in [0, 1]$ controls structure/sequence balance. We use $\alpha=0.7$ (70\% structure, 30\% sequence) based on preliminary experiments, emphasizing structural similarity for OOD generalization—particularly in the sequence identity ``twilight zone'' (20--30\%) where structure-based methods excel.

\subsection{Method 4: Enhanced Prompt Engineering}

Building on Weighted Similarity ($\alpha=0.7$), we optimize prompts for Gemini 2.5 Flash through four enhancements:

\textbf{(1) Task-Specific Instructions:} Prime LLM with domain context. Example for Domain/Motif: ``Identify structural domains, functional motifs, binding sites, and conserved sequence patterns.''

\textbf{(2) Confidence Signals:} Present retrieval with explicit confidence levels and scores:
\begin{verbatim}
[Rank 1] Confidence: High (0.87)
Annotation: {annotation_1}
\end{verbatim}
This enables LLM to weight evidence appropriately.

\textbf{(3) Few-Shot Examples:} Include 2--3 training examples (nearest neighbors to query) demonstrating desired output format and biological terminology.

\textbf{(4) JSON Constraints:} Require structured output (\texttt{\{"description": "..."\}}) for consistent parsing and reduced hallucination.

\subsection{Evaluation Metrics}

\textbf{Meta-BLEU} (primary): Extract biological entities from predictions/ground truth, compute BLEU on entity n-grams:
\begin{equation}
\text{Meta-BLEU} = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
\end{equation}
Report Meta-BLEU-2 (bigrams) and Meta-BLEU-4 (up to 4-grams) to capture multi-word terms like ``ATP-binding cassette transporter.''

\textbf{Meteor} (secondary): Semantic similarity with synonym matching \cite{banerjee2005meteor}:
\begin{equation}
\text{Meteor} = F_{\text{mean}} \cdot (1 - \text{Penalty})
\end{equation}

\subsection{Implementation Details}

\begin{itemize}
    \item \textbf{LLM}: Gemini 2.5 Flash (temperature=0.7, top\_p=0.9, max\_tokens=512)
    \item \textbf{Embeddings}: ESM-2 (1280-dim), ProstT5 (1024-dim), L2-normalized
    \item \textbf{Retrieval}: Faiss HNSW, Top-$K=10$
    \item \textbf{Evaluation}: 256 samples/task, 3 runs (seeds 0, 42, 123), averaged results
\end{itemize}
